{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf55f1d6-5ccd-4896-ba93-b5e40fa8851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports chargés ✅\n",
      "Liens PDF récupérés : ['https://arxiv.org/pdf/2507.04370', 'https://arxiv.org/pdf/2507.04299', 'https://arxiv.org/pdf/2507.05246', 'https://arxiv.org/pdf/2507.04431', 'https://arxiv.org/pdf/2507.04381', 'https://arxiv.org/pdf/2507.04600', 'https://arxiv.org/pdf/2507.03916', 'https://arxiv.org/pdf/2507.04770', 'https://arxiv.org/pdf/2507.04283', 'https://arxiv.org/pdf/2507.05241']\n",
      "Téléchargement 1: https://arxiv.org/pdf/2507.04370\n",
      "Téléchargement 2: https://arxiv.org/pdf/2507.04299\n",
      "Téléchargement 3: https://arxiv.org/pdf/2507.05246\n",
      "Téléchargement 4: https://arxiv.org/pdf/2507.04431\n",
      "Téléchargement 5: https://arxiv.org/pdf/2507.04381\n",
      "Téléchargement 6: https://arxiv.org/pdf/2507.04600\n",
      "Téléchargement 7: https://arxiv.org/pdf/2507.03916\n",
      "Téléchargement 8: https://arxiv.org/pdf/2507.04770\n",
      "Téléchargement 9: https://arxiv.org/pdf/2507.04283\n",
      "Téléchargement 10: https://arxiv.org/pdf/2507.05241\n",
      "Téléchargement terminé ✅\n"
     ]
    }
   ],
   "source": [
    "# Génération de sommaires pour articles scientifiques (cas AFNOR)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "arxiv_url = \"https://arxiv.org/list/cs.AI/recent\"\n",
    "\n",
    "## 📥 Étape 1 : Téléchargement des articles depuis arXiv\n",
    "output_dir = \"articles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Imports chargés ✅\")\n",
    "\n",
    "max_article_count = 10\n",
    "\n",
    "response = requests.get(arxiv_url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extraire les liens de téléchargement PDF\n",
    "pdf_links = []\n",
    "for link in soup.find_all(\"a\"):\n",
    "    href = link.get(\"href\", \"\")\n",
    "    if href.startswith(\"/pdf/\") and \"format\" not in href:\n",
    "        full_link = \"https://arxiv.org\" + href\n",
    "        pdf_links.append(full_link)\n",
    "\n",
    "# Enlever les doublons et limiter le nombre pour test\n",
    "pdf_links = list(set(pdf_links))[:max_article_count]  # Télécharge les X premiers articles\n",
    "print(\"Liens PDF récupérés :\", pdf_links)\n",
    "\n",
    "\n",
    "for i, pdf_url in enumerate(pdf_links, start=1):\n",
    "    print(f\"Téléchargement {i}: {pdf_url}\")\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        pdf_name = pdf_url.split(\"/\")[-1] + \".pdf\"\n",
    "        with open(os.path.join(output_dir, pdf_name), \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur pendant le téléchargement de {pdf_url} : {e}\")\n",
    "\n",
    "print(\"Téléchargement terminé ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf7bfbb-316f-4c43-945e-8e243dd627b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TOC extrait : 2507.05246.pdf -> output_v1/2507.05246.md\n",
      "✅ TOC extrait : 2507.04370.pdf -> output_v1/2507.04370.md\n",
      "✅ TOC extrait : 2507.04600.pdf -> output_v1/2507.04600.md\n",
      "✅ TOC extrait : 2507.04770.pdf -> output_v1/2507.04770.md\n",
      "✅ TOC extrait : 2507.05241.pdf -> output_v1/2507.05241.md\n",
      "✅ TOC extrait : 2507.04299.pdf -> output_v1/2507.04299.md\n",
      "✅ TOC extrait : 2507.03916.pdf -> output_v1/2507.03916.md\n",
      "✅ TOC extrait : 2507.04283.pdf -> output_v1/2507.04283.md\n",
      "✅ TOC extrait : 2507.04381.pdf -> output_v1/2507.04381.md\n",
      "✅ TOC extrait : 2507.04431.pdf -> output_v1/2507.04431.md\n"
     ]
    }
   ],
   "source": [
    "## Approche 1 : Extraction simple de texte + Expressions regex\n",
    "# !pip install PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === PARAMÈTRES ===\n",
    "INPUT_DIR = \"articles\"\n",
    "OUTPUT_DIR = \"output_v1\"\n",
    "MAX_TITLE_WORDS = 12\n",
    "\n",
    "# === REGEX PATTERNS ===\n",
    "section_pattern = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)(?:\\.|\\))?\\s+([A-Z][\\w\\s\\-,:;\\(\\)]*)$\")\n",
    "roman_pattern = re.compile(r\"^\\s*(?=[IVXLCDM]+\\.)\\s*([IVXLCDM]+)\\.\\s+([A-Z][\\w\\s\\-,:;\\(\\)]*)$\", re.IGNORECASE)\n",
    "appendix_pattern = re.compile(r\"^\\s*([A-Z])(\\.|[\\s])\\s+([A-Z\\s\\-]{5,})$\")\n",
    "uppercase_pattern = re.compile(r\"^[A-Z\\s\\-]{5,}$\")\n",
    "\n",
    "# === UTILITAIRES ===\n",
    "\n",
    "def is_valid_title(line: str) -> bool:\n",
    "    return bool(line) and len(line.split()) <= MAX_TITLE_WORDS and line[-1] not in \".:;!?\"\n",
    "\n",
    "def clean_title_line(line: str) -> str:\n",
    "    return line.strip()\n",
    "\n",
    "def extract_toc_from_text(text: str):\n",
    "    toc = []\n",
    "    for line in text.splitlines():\n",
    "        raw_line = line.strip()\n",
    "        if not is_valid_title(raw_line):\n",
    "            continue\n",
    "\n",
    "        for pattern in [section_pattern, roman_pattern, appendix_pattern]:\n",
    "            match = pattern.match(raw_line)\n",
    "            if match:\n",
    "                parts = match.groups()\n",
    "                number, title = parts[0], parts[-1]\n",
    "                toc.append((number, clean_title_line(title)))\n",
    "                break\n",
    "        else:\n",
    "            if uppercase_pattern.match(raw_line):\n",
    "                toc.append((\"\", clean_title_line(raw_line)))\n",
    "    return toc\n",
    "\n",
    "def save_toc_to_md(toc, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Table des matières détectée\\n\\n\")\n",
    "        for number, title in toc:\n",
    "            indent = \"  \" * number.count(\".\") if \".\" in number else \"\"\n",
    "            line = f\"{indent}- {number + ' ' if number else ''}{title}\"\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "# === TRAITEMENT EN LOT ===\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "pdf_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(INPUT_DIR, pdf_file)\n",
    "    output_path = os.path.join(OUTPUT_DIR, os.path.splitext(pdf_file)[0] + \".md\")\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\\n\".join(page.get_text() for page in doc)\n",
    "        toc = extract_toc_from_text(full_text)\n",
    "        save_toc_to_md(toc, output_path)\n",
    "        print(f\"✅ TOC extrait : {pdf_file} -> {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec {pdf_file} : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03340e82-c544-4a72-a740-bf584273539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: 2507.05246.pdf → output_v2/2507.05246/toc_final.json (88 entries)\n",
      "✅ Done: 2507.04370.pdf → output_v2/2507.04370/toc_final.json (29 entries)\n",
      "✅ Done: 2507.04600.pdf → output_v2/2507.04600/toc_final.json (30 entries)\n",
      "✅ Done: 2507.04770.pdf → output_v2/2507.04770/toc_final.json (25 entries)\n",
      "✅ Done: 2507.05241.pdf → output_v2/2507.05241/toc_final.json (18 entries)\n",
      "✅ Done: 2507.04299.pdf → output_v2/2507.04299/toc_final.json (11 entries)\n",
      "✅ Done: 2507.03916.pdf → output_v2/2507.03916/toc_final.json (26 entries)\n",
      "✅ Done: 2507.04283.pdf → output_v2/2507.04283/toc_final.json (20 entries)\n",
      "✅ Done: 2507.04381.pdf → output_v2/2507.04381/toc_final.json (18 entries)\n",
      "✅ Done: 2507.04431.pdf → output_v2/2507.04431/toc_final.json (9 entries)\n"
     ]
    }
   ],
   "source": [
    "## Approche 2 : Extraction à l'aide de modèles LLM\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY1\").strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "model = \"gpt-4.1\"\n",
    "articles_dir = \"articles\"\n",
    "output_dir = \"output_v2\"\n",
    "# max_pages = 100\n",
    "delay_between_calls = 1.2  # seconds\n",
    "\n",
    "# === UTILS ===\n",
    "\n",
    "def get_page_text(path, page_num):\n",
    "    doc = fitz.open(path)\n",
    "    if page_num >= len(doc):\n",
    "        return None\n",
    "    page = doc.load_page(page_num)\n",
    "    return page.get_text()\n",
    "\n",
    "def build_prompt(text, page_number):\n",
    "    return f\"\"\"\n",
    "You are analyzing a scientific research paper.\n",
    "\n",
    "This is the content of page {page_number}. Your task is to extract any section or subsection titles.\n",
    "\n",
    "If titles like \"1 Introduction\" or \"2.1 Related Work\" are present, return them in this JSON format:\n",
    "\n",
    "[\n",
    "  {{\"section\": \"1 Introduction\", \"page\": {page_number}}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "If no headers are found, return an empty list: []\n",
    "\n",
    "Only return valid JSON. No commentary.\n",
    "\n",
    "--- PAGE {page_number} ---\n",
    "{text}\n",
    "--- END PAGE ---\n",
    "\"\"\"\n",
    "\n",
    "def call_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def clean_json_string(raw_text):\n",
    "    cleaned = raw_text.strip().strip(\"```\").replace(\"json\", \"\").strip()\n",
    "    match = re.search(r\"\\[.*\\]\", cleaned, re.DOTALL)\n",
    "    return match.group(0).strip() if match else cleaned\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# === TRAITEMENT PRINCIPAL ===\n",
    "\n",
    "def process_pdf(pdf_path, output_subdir):\n",
    "    toc = []\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    article_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # print(f\"\\n📘 Processing {article_name}...\")\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        num_pages = len(doc)\n",
    "    \n",
    "    for i in range(num_pages):\n",
    "        page_number = i + 1\n",
    "        # print(f\"  📄 Page {page_number}...\")\n",
    "\n",
    "        text = get_page_text(pdf_path, i)\n",
    "        if not text or len(text.strip()) < 20:\n",
    "            # print(\"   ⚠️ Skipped (empty or short text)\")\n",
    "            continue\n",
    "\n",
    "        prompt = build_prompt(text, page_number)\n",
    "\n",
    "        try:\n",
    "            raw = call_gpt(prompt)\n",
    "            # print(f\"   📝 Raw GPT output:\\n{raw}\\n\")\n",
    "\n",
    "            cleaned = clean_json_string(raw)\n",
    "            entries = json.loads(cleaned)\n",
    "            if isinstance(entries, list):\n",
    "                toc.extend(entries)\n",
    "            else:\n",
    "                print(\"   ❌ Unexpected JSON structure.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"   ❌ JSON parse error — saving raw output.\")\n",
    "            with open(os.path.join(output_subdir, f\"debug_page_{page_number}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(raw)\n",
    "        except Exception as e:\n",
    "            print(f\"   🚨 Error: {e}\")\n",
    "\n",
    "        # Sauvegarde intermédiaire\n",
    "        intermediate_path = os.path.join(output_subdir, \"toc_intermediate.json\")\n",
    "        save_json(toc, intermediate_path)\n",
    "        # print(f\"   💾 Intermediate saved: {len(toc)} entries\")\n",
    "\n",
    "        time.sleep(delay_between_calls)\n",
    "\n",
    "    return toc\n",
    "\n",
    "# === SCRIPT GLOBAL ===\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdf_files = [f for f in os.listdir(articles_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(articles_dir, pdf_file)\n",
    "        article_name = os.path.splitext(pdf_file)[0]\n",
    "        output_subdir = os.path.join(output_dir, article_name)\n",
    "        os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "        toc = process_pdf(pdf_path, output_subdir)\n",
    "\n",
    "        final_path = os.path.join(output_subdir, \"toc_final.json\")\n",
    "        save_json(toc, final_path)\n",
    "\n",
    "        print(f\"✅ Done: {pdf_file} → {final_path} ({len(toc)} entries)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efda36-64dc-44a0-8856-6f8afab6f53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
